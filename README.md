# Nuclear-IT-hack-24
Репозиторий решения команды AXIOM на отборочном этапе _Nuclear IT Hack 2024_

# Задача

В рамках хакатона команда решала задачу от **МТС Линк**.

Постановка задачи: Разработать систему на основе ИИ, которая анализирует список пользовательских ответов возвращает понятное и интерпретируемое облако слов.

# Команда: AXIOM
- Зворыгин Владимир Андреевич
- Миронов Андрей Михайлович
- Диков Александр Евгеньевич
- Садохин Алексей Александрович
- 
# Структура проекта
- **_app_**: streamlit приложение, показывающее демонстрацию работы нашего алгоритма
- **_datasets_**: сгенерированные датасеты для тестирования разработанного алгоритма
- **_experiments_**: ноутбуки с экспериментами и тестированием моделей

# Идея решения

Для того, чтобы получить облако слов из большого числа пользовательских ответов, необходимо эти ответы агрегировать. Агрегировать напрямую слова проблематично. Необходимо провести предобработку слов и преобразовать их в числа, а именно вектора, чтобы далее их объединять/кластеризовать. 

Для работы необходимо иметь датасеты. Для этого были сгенерировали 4 базы ответов разного размера и немного отличающегося наполнения. 

При обработке данные датасета объединились в один массив. В рамках этого массива все буквы приводились к нижнему регистру. Затем было проведено удаление стоп-слов (как на русском, так и на английском), а также лемматизация.

После таких операций в массиве стало много повторяющихся слов и выражений. Мы составили словарь с подсчётом количества каждого объекта. Далее для каждого ключа получившегося словаря были получены эмбеддинги. 

В качестве эмбеддинг моделей были рассмотрены предобученные bert-like модели: 
- [_ru-en-RoSBERTa_](https://huggingface.co/ai-forever/ru-en-RoSBERTa)
- [_ruBert-large_](https://huggingface.co/ai-forever/ruBert-large)
- [_ruElectra-large_](https://huggingface.co/ai-forever/ruElectra-large)

И более простой метод векторизации текста word2vec - для обучения был взят корпус [русских текстов](https://huggingface.co/Word2vec/wikipedia2vec_enwiki_20180420_300d). Данный метод показал наибольшую эффективность по скорости, но меньшее качество при построении векторов и дальнейшей кластеризации.

Поэтому для решения в основном были использованы bert-like модели. В рамках модели **_ru-en-RoSBERTa_**, согласно документации, при получении эмбеддингов был использован префикс "_clustering:_ " для повышения точности. 

Для кластеризации были использованы 2 алгоритма - _**AgglomerativeClustering**_ и _**DBSCAN**_, так как эти алгоритмы не требуют задания числа кластеров. Были проведены различные эксперименты с этими алгоритмами с использованием различных моделей на разных датасетах. 
Также для визуализации полученные на маленьком датасете эмбеддинги мы понизили в размерности до 2 с помощью **_PCA_** и отобразили получающиеся кластеры.  

В результате экспериментов была выбрана модель **_ru-en-RoSBERTa_** (как наиболее точная) и алгоритм **_AgglomerativeClustering_** для кластеризации. Также была рассмотрена обученная нами модель **_word2vec_**. Для них было проведено тестирование производительности на всех датасетах - результаты представлены в экспериментах.x  

# Демо решения


# Разворачивание решения
